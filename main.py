import os
from os.path import dirname, join
from dotenv import load_dotenv
from io import BytesIO
import traceback
import streamlit as st
import openai
import time
from datetime import datetime, timezone
import json
import time
import base64
import re
from functools import reduce
from mimetypes import guess_type
from openai import AssistantEventHandler, AzureOpenAI
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from typing_extensions import override
from dataclasses import dataclass, field
from typing import Any, List, Dict, Optional, Union
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessage,
    ChatCompletionChunk
)
from openai.types.beta.threads import (
    TextContentBlock,
    ImageURLContentBlock,
    ImageFileContentBlock,
    ImageFile,
    ImageURL,
    Text,
    Annotation,
    Run
)
import concurrent.futures
import customTools
import serperTools
import internetAccess

@dataclass
class GPTHallucinatedFunctionCall:
    tool_uses: List['HallucinatedToolCalls']
    def __post_init__(self):
        self.tool_uses = [HallucinatedToolCalls(**i) for i in self.tool_uses]

@dataclass
class HallucinatedToolCalls:
    recipient_name: str
    parameters: dict

dotenv_path = join(dirname(__file__), ".env.local")
load_dotenv(dotenv_path)

tools=[{"type": "code_interpreter"}, {"type": "file_search"}, customTools.time, serperTools.run, serperTools.results, serperTools.scholar, serperTools.news, serperTools.places, internetAccess.html]

class StreamHandler(AssistantEventHandler):
    @override
    def __init__(self, client):
        super().__init__()
        self.client = client
        # è¦ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨æœ€çµ‚çš„ãªrunã‚’å¼•ãæ¸¡ã™ï¼ˆtool_callã”ã¨ã«æ–°ã—ã„å­ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒç”Ÿã˜ã‚‹ï¼‰
        self.content = []
        self.final_run = None

    @override
    def on_event(self, event):
      if event.event == 'thread.run.requires_action':
          run_id = event.data.id
          self.handle_requires_action(event.data, run_id)

    @override
    def on_image_file_done(self, image_file: ImageFile) -> None:
        print("on_image_file_done ImageFile:", image_file)
        self.content.append(ImageFileContentBlock(type="image_file", image_file=image_file))
        st.image(get_file(image_file.file_id))

    @override
    def on_text_done(self, text: Text) -> None:
        print("on_text_done Text:", text)
        self.content.append(TextContentBlock(type="text", text=text))
        value, files = parse_annotations(text.value, text.annotations)
        put_buttons(files, "stream")

    @override
    def on_tool_call_created(self, tool_call: Any) -> None:
        print(f"\nassistant > tool_call_created > {tool_call.type}\n", flush=True)
        if tool_call.type != "function":
            st.toast(tool_call.type)
        print(tool_call, flush=True)

    @override
    def on_tool_call_delta(self, delta: Any, snapshot: Any) -> None:
        if delta.type == "code_interpreter":
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="", flush=True)
            if delta.code_interpreter.outputs:
                print("\n\noutput >", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        print(f"\n{output.logs}", flush=True)

    def handle_requires_action(self, data, run_id):
        print(f"\nassistant > {data}\n", flush=True)
        tool_calls = data.required_action.submit_tool_outputs.tool_calls

        tool_outputs = handle_tool_calls(tool_calls)

        with self.client.beta.threads.runs.submit_tool_outputs_stream(
            thread_id=self.current_run.thread_id,
            run_id=self.current_run.id,
            tool_outputs=tool_outputs,
            event_handler=StreamHandler(self.client)
        ) as stream:
            st.write_stream(stream.text_deltas)
            stream.until_done()
        # AssistantEventHandlerã«çµ„ã¿è¾¼ã¿ã®ã‚¤ãƒ™ãƒ³ãƒˆæ©Ÿæ§‹ã«ã‚ˆã‚Šã€thread.run.completed, canceled, 
        # expired, failed, required_action, incompleteã®éš›ã«ã€__current_runãŒæ›´æ–°ã•ã‚Œã€
        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£current_run()ã«ã‚ˆã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹
        self.final_run = stream.final_run or stream.current_run
        self.content += stream.content


# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®å®šç¾©
@dataclass
class ChatMessage:
    role: str
    # contentã¯Assistant APIã®contentå®šç¾©ã‚’å€Ÿç”¨
    content: List[Union[ImageFileContentBlock, ImageURLContentBlock, TextContentBlock]]
    files: List[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)

# ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹
class ChatThread:
    def __init__(self, client):
        self.client = client
        self.messages = []
        self.thread_id = None

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        if isinstance(content, str):
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]

        self.messages.append(ChatMessage(role, content, files, metadata))

        # gpt-4oã®Assistant APIã§ç”»åƒèªè­˜ãŒå‡ºæ¥ãªã„å•é¡Œã¯ã¨ã‚Šã‚ãˆãšãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨ã™ã‚‹
        # Assistant APIã¯ImageURLContentBlockã‚’èªè­˜ã—ãªã„ï¼Ÿã™ã‚‹ã¯ãšã ãŒãƒ»ãƒ»

        # Assistant APIã‚’æœªä½¿ç”¨ã®æ®µéšã§ã¯thread_idã¯å­˜åœ¨ã—ãªã„ã€‚åˆã‚ã¦ä½¿ã†æ™‚ã«ä½œæˆã—ã¦éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        # Assistant APIæ™‚ã«ã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯è‡ªå‹•çš„ã«Threadã«è¨˜éŒ²ã•ã‚Œã‚‹ã€‚
        if self.thread_id and (model["api_mode"] != "assistant" or role != "assistant"):
            self.client.beta.threads.messages.create(
                thread_id = self.thread_id,
                role = role,
                content = self.content_to_content_param(content),
                attachments = files
            )

    def get_last_message(self):
        return self.messages[-1]

    def get_thread_id(self):
        """
        Assistant APIç”¨ã®thread_idã‚’è¿”ã™ã€‚åˆã‚ã¦Assistant APIã‚’ä½¿ã†ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§
        threadã‚’ä½œæˆã—ã€éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹
        """
        if self.thread_id:
            return self.thread_id

        thread = self.client.beta.threads.create(
            messages = [
                {
                    "role": msg.role,
                    # ToDo: 32ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»¥ä¸Šæºœã¾ã£ã¦ã‹ã‚‰ã ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚
                    "content": self.content_to_content_param(msg.content),
                    "attachments": msg.files
                }
                for msg in self.messages
            ]
        )
        self.thread_id = thread.id
        
        return self.thread_id

    @staticmethod
    def content_to_content_param(content: List[Union[ImageFileContentBlock, ImageURLContentBlock, TextContentBlock]]) -> List[dict]:
        """
        ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã®contentã‚’ã€APIé€ä¿¡ç”¨ã«dictã«å¤‰æ›ã™ã‚‹
        """
        content_param = []
        for block in content:
            if block.type == "text":
                # ã“ã®ã‚ãŸã‚Šã€å¾®å¦™ã«ä¸€å¯¾ä¸€é–¢ä¿‚ã§ã¯ãªã„
                content_param.append({
                    "type": block.type,
                    "text": block.text.value
                })
            elif block.type == "image_file":
                content_param.append({
                    "type": block.type,
                    "image_file": {"file_id": block.image_file.file_id, "detail": block.image_file.detail},
                })
            elif block.type == "image_url":
                content_param.append({
                    "type": block.type,
                    "image_url": {"url": block.image_url.url, "detail": block.image_url.detail},
                })
            else:
                raise ValueError(f"æœªçŸ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã® type: {block.type}")
        return content_param

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹
class ConversationManager:
    def __init__(self, clients, assistants):
        self.client = clients["openai"]
        self.thread = ChatThread(self.client)
        self.assistants = assistants

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ChatThreadã«è¿½åŠ """
        self.thread.add_message(model, role, content, files, metadata)

    def get_completion_messages(self, model, text_only=False):
        """Completion APIç”¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰æ›"""
        messages = []
        for msg in self.thread.messages:
            # Assistant APIç”¨ã®ImageFileContentBlockã¯é™¤ã
            content = [cont for cont in msg.content if not isinstance(cont, ImageFileContentBlock)]

            # Visionã‚µãƒãƒ¼ãƒˆã®ç„¡ã„ãƒ¢ãƒ‡ãƒ«ã«Imageã‚’ä¸ãˆã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§é™¤ã
            if not model.get("support_vision", False):
                content = [cont for cont in content if isinstance(cont, TextContentBlock)]

            if text_only:
                # Deepseekãªã©ã€ãƒ†ã‚­ã‚¹ãƒˆã ã‘å¿…è¦ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
                content = "\n".join([cont.text.value for cont in content if isinstance(cont, TextContentBlock)])
            else:
                # ãã†ã§ãªã„å ´åˆã¯classã‹ã‚‰dictã«å¤‰æ›ã™ã‚‹
                content = self.thread.content_to_content_param(content)

            messages.append({
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": content
            })
        return messages

    def create_attachments_for_assistant(self, files, tool_for_files):
        """Assistantç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        attachments = []
        for file in files:
            file.seek(0)
            response = self.client.files.create(
                file=file,
                purpose="assistants"
            )
            attachments.append(
                    {
                        "file_id": response.id,
                        "tools": [{"type": tool_for_files}],
                    }
            )
        return attachments

    def create_ImageURLContentBlock(self, file, detail_level):
        mime_type = guess_type(file.name)[0]
        image_encoded = base64.b64encode(file.getvalue()).decode()
        image_url = f'data:{mime_type};base64,{image_encoded}'
        return ImageURLContentBlock(
            type="image_url",
            image_url=ImageURL(url=image_url, detail=detail_level)
        )

# gpt-4oãŒAssistant APIã§ç”»åƒã‚’èªè­˜ã—ãªã„ã®ã§ã€ImageFileãªã‚‰ã¨æ€ã£ã¦åŠ ãˆãŸãŒã€ã©ã†ã‚„ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ–¹ã®å•é¡Œã‚‰ã—ã„
#    def create_ImageFileContentBlock(self, file, detail_level):
#        response = self.client.files.create(
#            file=file,
#            purpose="vision"
#            # "vision"ã‚’æŒ‡å®šã™ã‚‹ã¨ã€"purpose contains an invalid purpose vision"ã¨è¨€ã‚ã‚Œã¦ã—ã¾ã†ã€‚
#            # Azureã®APIãŒè¿½ã„ã¤ã„ã¦ã„ãªã„å¯èƒ½æ€§ã‚ã‚Šã€‚"assistant"ãªã‚‰å—ã‘ä»˜ã‘ã‚‹ãŒã€gpt-4oã¯è‡ªåˆ†ã«ã¯ç”»åƒèªè­˜èƒ½åŠ›ãŒç„¡ã„ã¨è¨€ã†
#        )
#        return ImageFileContentBlock(
#            type="image_file",
#            image_file=ImageFile(file_id=response.id, detail=detail_level)
#        )

def pretty_print(messages: List[ChatMessage]) -> None:
    i = -1
    m = None
    for i0, m0 in enumerate(messages):
#        print("role:", m.role)
#        print("content:", m.content)
        if i != -1:
            with st.chat_message("assistant" if m.role == "assistant" else "user"):
                pretty_print_message(i, m)
        i = i0
        m = m0

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿token_summaryã‚’è¡¨ç¤º
    if i != -1:
        with st.chat_message("assistant" if m.role == "assistant" else "user"):
            pretty_print_message(i, m, with_token_summary=True)


def pretty_print_message(key, message, with_token_summary=False):
    for j, cont in enumerate(message.content):
        if isinstance(cont, ImageFileContentBlock) and message.role == "assistant":
            # è‡ªåˆ†ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã®ã§ã€asssistantã®å ´åˆã®ã¿è¡¨ç¤º
            st.image(get_file(cont.image_file.file_id))
        if isinstance(cont, ImageURLContentBlock):
            st.image(cont.image_url.url)
        if isinstance(cont, TextContentBlock):
            value, files = parse_annotations(cont.text.value, cont.text.annotations)
            st.markdown(value, unsafe_allow_html=True)
            put_buttons(files, f"hist{key}-{j}")
    print(message)
    print(with_token_summary)
    if "file_search_results" in message.metadata:
        put_quotations(message.content, message.metadata["file_search_results"])
    if with_token_summary and "token_summary" in message.metadata:
        st.markdown(message.metadata["token_summary"])

def put_buttons(files, key=None) -> None:
    for i, file in enumerate(files):
        if key:
            key=f"{key}-{i}"
        else:
            key = None
        if file["type"] == "file_path":
            st.download_button(
                f"{file["index"]}: {file["filename"]} : ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                get_file(file["file_id"]),
                file_name=file["filename"],
                key=key
            )

def put_quotations(content, file_search_results):
    citations = {}
    for cont in content:
        if isinstance(cont, TextContentBlock):
            for annotation in cont.text.annotations:
                if annotation.type == "file_citation" and (match := re.search(r'(\d+):(\d+)', annotation.text)):
                    i = int(match[2])
                    if i not in citations:
                        citations[i] = annotation.text
    for i, text in citations.items():
        result = file_search_results[i]
        with st.expander(f"{text}: {result.file_name}"):
            st.write(f"""
~~~
score: {result.score}
~~~
{result.content[0].text}
""")

def get_file(file_id: str) -> bytes:
    key = f"content_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openai"]
    retrieve_file = client.files.with_raw_response.content(file_id)
    content: bytes = retrieve_file.content
    st.session_state.fileCache[key] = content
    return content

def get_file_info(file_id: str) -> bytes:
    key = f"info_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openai"]
    retrieve_file = client.files.retrieve(file_id)
    st.session_state.fileCache[key] = retrieve_file
    return retrieve_file

def parse_annotations(value: str, annotations: List[Annotation]):
    files = []
    print(value)
    print(annotations)
    for (
        index,
        annotation,
    ) in enumerate(annotations):
        # FilePathAnnotation
        if annotation.type == "file_path":
            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_path.file_id,
                    "filename": annotation.text.split("/")[-1],
                    "text": annotation.text,
                    "index": index
                }
            )
        elif annotation.type == "file_citation":
            info = get_file_info(annotation.file_citation.file_id)
            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_citation.file_id,
                    "filename": info.filename,
                    "text": annotation.text,
                    "index": index
                }
            )
    value = re.sub(r'\[([^\]]*)\]\((sandbox:[^)]*)\)', r'ãƒœã‚¿ãƒ³\1', value)
    return value, files

def handle_tool_calls(tool_calls: List[Dict], mode = "assistant") -> List[Dict]:
    """
    ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
    """
    print(tool_calls)
    def add_output(outputs, tool_call_id, output, mode, fname = None):
        if mode == "assistant":
            # Assistant APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "output": output
            })
        else:
            # Completion APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "role": "tool",
                "name": fname,
                "content": output,
            })

    tool_outputs = []

    for tool in tool_calls:
        function = tool.function
        fname = function.name
# ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã«éƒ½åˆã‚ˆãç™ºç”Ÿã—ã¦ãã‚Œãªã„ã®ã§ï¼‰
#        function.name = "multi_tool_use.parallel"
#        function.arguments = '{"tool_uses": [{"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 processor"}}, {"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 chip"}}]}'
        fargs = json.loads(function.arguments)
        print(f"Function call: {fname}")
        print(f"Function arguments: {fargs}")

        if function.name == "multi_tool_use.parallel":
            # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼šAIãŒä¸¦åˆ—å®Ÿè¡Œã‚’è¦æ±‚ã—ã¦ãã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
            # ä»•æ§˜å¤–ã®å‹•ä½œ(ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³)ã ã¨ã„ã†èª¬ã‚‚ã‚ã‚‹ã€‚å‹•ä½œæ¤œè¨¼æœªæ¸ˆã€‚
            # We need to deserialize the arguments
            caught_calls = GPTHallucinatedFunctionCall(**(json.loads(function.arguments)))
            tool_uses = caught_calls.tool_uses

            # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
            with concurrent.futures.ThreadPoolExecutor() as executor:

                # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œç”¨ã‚¿ã‚¹ã‚¯ã«å¤‰æ›
                future_to_tool = {
                    executor.submit(
                        function_calling,
                        tool_use.recipient_name.rsplit('.', 1)[-1],
                        tool_use.parameters
                    ): {"id": tool.id, "fname": tool_use.recipient_name.rsplit('.', 1)[-1]}
                    for tool_use in tool_uses
                }

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’åé›†
                results = []
                for future in concurrent.futures.as_completed(future_to_tool):
                    tool_call_id = future_to_tool[future]["id"]
                    fname = future_to_tool[future]["fname"]
                    print(f"fname: {fname}")
                    print(f"tool_call_id: {tool_call_id}")
                    try:
                        results.append(future.result())
                    except Exception as e:
                        print(f"Error: {str(e)}")
                        results.append(json.dumps(f"Error: {str(e)}"))

                print(results)
                # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«ã¤ã„ã¦ã€tool_call_idã¯ä¸€ã¤ã—ã‹ãªãã€jsonã‚’æ”¹è¡Œã§é€£çµã—ä¸€ã¤ã®
                # tool_outputã«ã¾ã¨ã‚ã¦è¿”ã™(jsonl?)ã€‚ã“ã®æƒ…å ±ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä¼šè©±ã‚ˆã‚Šå¾—ãŸãŒæ­£ã—ã„ã‹åˆ†ã‹ã‚‰ãªã„ã€‚
                add_output(tool_outputs, tool_call_id, "\n".join(results), mode, fname)
        else:
              # é †æ¬¡ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
              fresponse = function_calling(fname, fargs)
              add_output(tool_outputs, tool.id, fresponse, mode, fname)

    print("tool_outputs:")
    print(tool_outputs)
    return tool_outputs

def function_calling(fname, fargs):
        print(f"fc fname: {fname}")
        print(f"fc fargs: {fargs}")
        if fname == "get_current_weather":
            fresponse = customTools.get_current_weather(
                location=fargs.get("location"),
            )
        elif fname == "get_current_datetime":
            st.toast("[datetime]", icon="ğŸ•’");
            fresponse = customTools.get_current_datetime(
                timezone=fargs.get("timezone")
            )
        elif fname == "get_google_serper":
            st.toast(f"[Google Serper] {fargs.get("query")}", icon="ğŸ”");
            fresponse = serperTools.get_google_serper(
                query=fargs.get("query")
            )
        elif fname == "get_google_results":
            st.toast(f"[Google detail] {fargs.get("query")}", icon="ğŸ”");
            fresponse = serperTools.get_google_results(
                query=fargs.get("query")
            )
        elif fname == "get_google_scholar":
            st.toast(f"[Google scholar] {fargs.get("query")}", icon="ğŸ“");
            fresponse = serperTools.get_google_scholar(
                query=fargs.get("query")
            )
        elif fname == "get_google_news":
            st.toast(f"[Google news] {fargs.get("query")}", icon="ğŸ“°");
            fresponse = serperTools.get_google_news(
                query=fargs.get("query")
            )
        elif fname == "get_google_places":
            st.toast(f"[Google places] {fargs.get("query")}", icon="ğŸ½ï¸");
            fresponse = serperTools.get_google_places(
                query=fargs.get("query"),
                country=fargs.get("country", "jp"),
                language=fargs.get("language", "ja")
            )
        elif fname == "parse_html_content":
            st.toast("[parse html content]", icon="ğŸ‘€");
            fresponse = internetAccess.parse_html_content(
                url=fargs.get("url"),
                query=fargs.get("query", "headings"),
                heading=fargs.get("heading", None)
            )
        return fresponse

# APIå®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
def execute_api(model, selected_tools, conversation, options = {}):

    thread = conversation.thread
    client = model["client"]

    if model["api_mode"] == "inference":
        # DeepSeekã‚„Phiå‘ã‘ã®Inference API
        # https://learn.microsoft.com/en-us/rest/api/aifoundry/modelinference/
        messages = conversation.get_completion_messages(model, text_only=True)
        print(messages)
        try:
            if model["streaming"]:
                response = client.complete({
                    "stream": True,
                    "messages": messages,
# ç©ºã®toolsã‚’ä¸ãˆãŸã ã‘ã§ã‚‚ä¸å®‰å®šã«ãªã‚‹?ã„ã‚„ã€toolsã‚’ä¸ãˆãªãã¦ã‚‚ã“ã®ã‚¨ãƒ©ãƒ¼ã¯å‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ã‚µãƒ¼ãƒãƒ¼å´ã®æ··é›‘çŠ¶æ³ã«ã‚ˆã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿ DeepSeek APIã‚¨ãƒ©ãƒ¼: Operation returned an invalid status 'Too Many Requests' Content: Please check this guide to understand why this error code might have been returned
#                    "tools": [],
# ç¾æ™‚ç‚¹ã§function callingã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã€‚toolsã‚’æŒ‡å®šã™ã‚‹ã¨ã€åå¿œãŒæ­¢ã¾ã£ã¦ã—ã¾ã†ã€‚2025/2
# https://github.com/deepseek-ai/DeepSeek-R1/issues/9
#                    "tools": [customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html],
                    "model": model["model"],
                    "max_tokens": 4096
                })
                print(response)
                digester = completion_streaming_digester(response)
                full_response = st.write_stream(digester.generator)
                response = digester.response
                response_message = ChatCompletionMessage.model_validate(response["choices"][0])
                print(response)
                full_response = response_message.content
            else:
                response = client.complete({
                    "messages": messages,
                    "max_tokens": 4096
                })
                full_response = response.choices[0].message.content

            token_summary = get_token_summary(response, model)
            st.markdown(token_summary)
            metadata = {"token_summary": token_summary}
            return full_response, metadata

        except Exception as e:
            st.error(f"Azure AI Model Inference APIã‚¨ãƒ©ãƒ¼")
            raise

    elif model["api_mode"] == "assistant":
        thread_id = conversation.thread.get_thread_id()
        print(thread_id)

        args = {"thread_id": thread_id, "assistant_id": model["assistant_id"]} | options

        if model["support_tools"]: # selected_toolsãŒç©ºã®å ´åˆã‚‚assistantè¨­å®šã‚’ä¸Šæ›¸ã
            args["tools"] = selected_tools
            print(f"selected tools: {selected_tools}")

        if model["streaming"]:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Assistant APIå®Ÿè¡Œ
            args["event_handler"] = StreamHandler(client)
            try:
                print(args)
                with client.beta.threads.runs.stream(**args) as stream:
                    st.write_stream(stream.text_deltas)
                    stream.until_done()

                run = stream.final_run or stream.current_run
                content = stream.content
                print(content)
                print(run)
                file_search_results = get_file_search_results(thread_id, run.id)
                put_quotations(content, file_search_results)
                token_summary = get_token_summary(run, model)
                st.markdown(token_summary)
                metadata = {"token_summary": token_summary, "file_search_results": file_search_results}
                return content, metadata

            except Exception as e:
                st.error(f"Assistant(streaming) APIã‚¨ãƒ©ãƒ¼")
                raise

        else:
            # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆ Assistant APIã®å®Ÿè¡Œ
            try:
                # å®Ÿè¡Œé–‹å§‹
                run = client.beta.threads.runs.create(**args)

                # å®Ÿè¡Œå®Œäº†ã‚’å¾…æ©Ÿ
                while run.status not in ["completed", "failed"]:
                    print(run)
                    time.sleep(1)
                    run = client.beta.threads.runs.retrieve(
                        thread_id=thread.thread_id,
                        run_id=run.id
                    )
                    if run.status == "requires_action":
                        print(run.required_action)
                        messages = handle_tool_calls(run.required_action.submit_tool_outputs.tool_calls, "assistant")
                        run = client.beta.threads.runs.submit_tool_outputs(
                          thread_id=thread.thread_id,
                          run_id=run.id,
                          tool_outputs=messages
                        )

                if run.status == "failed":
                    raise Exception(f"å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {run.last_error}")

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
                print(run)
                messages = client.beta.threads.messages.list(
                    thread_id=thread.thread_id,
                    limit=1
                )
                print(messages)
                content = messages.data[0].content
                pretty_print_message("assist_msg", messages.data[0])
                token_summary = get_token_summary(run, model)
                st.markdown(token_summary)
                metadata = {"token_summary": token_summary}
                return content, metadata

            except Exception as e:
                st.error(f"Assistant APIã‚¨ãƒ©ãƒ¼")
                raise

    else:
        # Completion APIå®Ÿè¡Œå‡¦ç†
        messages = conversation.get_completion_messages(model)
        try:
            args = {"model": model["model"], "messages": messages} | options

            print(model)
            if model["streaming"]:
                args["stream"] = True
                args["stream_options"] = {"include_usage": True}

            if model["support_tools"] and selected_tools:
                args["tools"] = selected_tools
                print(f"selected tools: {selected_tools}")

            full_response = ""
            while True:
                print(f"args: {args}")
                response = client.chat.completions.create(**args)
                print(response)

                if model["streaming"]:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    digester = completion_streaming_digester(response)
                    full_response += st.write_stream(digester.generator)
                    response = digester.response
                    print(response)
                    response_message = ChatCompletionMessage.model_validate(response["choices"][0])

                else:
                    # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    response_message = response.choices[0].message
                    if hasattr(response_message, "content") and response_message.content:
                        st.write(response_message.content)
                        full_response += response_message.content
                    print(response)

                messages.append(response_message.model_dump())

                if hasattr(response_message, "tool_calls") and response_message.tool_calls:
                    messages += handle_tool_calls(response_message.tool_calls, "completion")

                else:
                    break
                
            token_summary = get_token_summary(response, model)
            st.markdown(token_summary)
            metadata = {"token_summary": token_summary}
            return full_response, metadata

        except Exception as e:
            st.error(f"Completion APIã‚¨ãƒ©ãƒ¼")
            raise

def get_file_search_results(thread_id, run_id):
    client = st.session_state.clients["openai"]

    # Run Stepã‚’å–å¾—
    run_steps = client.beta.threads.runs.steps.list(
        thread_id=thread_id,
        run_id=run_id
    )

    # æœ€å¾Œã®Run Stepã‹ã‚‰File Searchã®å®Ÿè¡Œçµæœã‚’å«ã‚ã¦å–å¾—ã™ã‚‹
    run_step = client.beta.threads.runs.steps.retrieve(
        thread_id=thread_id,
        run_id=run_id,
        step_id=run_steps.data[-1].id,
        include=["step_details.tool_calls[*].file_search.results[*].content"]
    )
    if hasattr(run_step.step_details, "tool_calls"):
        file_search_tcs = [tc for tc in run_step.step_details.tool_calls if hasattr(tc, "file_search")]
        if file_search_tcs:
            return file_search_tcs[0].file_search.results
    print(run_step)
    return []

def get_token_summary(response, model):
    if isinstance(response, ChatCompletion) or isinstance(response, Run):
        response = response.model_dump()
    token_summary = ""
    if "usage" in response and reduce(
        lambda a, c:c in response["usage"] and a,
        ["completion_tokens", "prompt_tokens", "total_tokens"], True):
        usage = response["usage"]
        token_summary = f"tokens in:{usage["prompt_tokens"]} out:{usage["completion_tokens"]} total:{usage["total_tokens"]}"
        if "pricing" in model:
            token_summary += f" cost: US${(usage["prompt_tokens"] * model["pricing"]["in"] + usage["completion_tokens"] * model["pricing"]["out"]) / 1000000}"
        token_summary = f"\n:violet-background[{token_summary}]"

    return token_summary 

class completion_streaming_digester:
    def __init__(self, stream):
        self.stream = stream
        self.response = {}

    def generator(self):
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŠã³tool_callã®æ–­ç‰‡ã‚’å—ã‘å–ã‚ŠãªãŒã‚‰UIã«åæ˜ ã—ã€æœ€çµ‚çš„ã«å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¾©å…ƒã™ã‚‹
        for chunk in self.stream:
            print(chunk)

            if isinstance(chunk, ChatCompletionChunk):
                # OpenAIã®Compltion APIã®å ´åˆ
                chunk_dict = chunk.model_dump()
            else:
                # deepseekãªã©
                chunk_dict = chunk

            for key, value in chunk_dict.items():
                if key == "choices":
                    if key not in self.response:
                        self.response[key] = {}
                    for choice in value:
                        for ckey, cvalue in choice.items():
                            ci = choice["index"]
                            if ci not in self.response[key]:
                                self.response[key][ci] = {"content": "", "tool_calls": {}}
                            if ckey == "delta":
                                for dkey, dvalue in cvalue.items():
                                    if dkey == "content" and dvalue:
                                        self.response[key][ci][dkey] += dvalue
                                        # UIã«å‡ºåŠ›
                                        yield dvalue
                                    elif dkey == "tool_calls" and dvalue:
                                        for tool_call in dvalue:
                                            for tkey, tvalue in tool_call.items():
                                                ti = tool_call["index"]
                                                if ti not in self.response[key][ci][dkey]:
                                                    self.response[key][ci][dkey][ti] = {"function": {"arguments":""}}
                                                if tkey == "function" and tvalue:
                                                    if "name" in tvalue and tvalue["name"]:
                                                        self.response[key][ci][dkey][ti][tkey]["name"] = tvalue["name"]
                                                    if "arguments" in tvalue and tvalue["arguments"]:
                                                        self.response[key][ci][dkey][ti][tkey]["arguments"] += tvalue["arguments"]
                                                elif tvalue:
                                                    self.response[key][ci][dkey][ti][tkey] = tvalue
                                    elif dvalue:
                                        self.response[key][ci][dkey] = dvalue
                            elif cvalue:
                                self.response[key][ci][ckey] = cvalue
                elif value:
                    self.response[key] = value
        # ä¸Šè¨˜ãƒãƒ¼ã‚¸ä½œæ¥­ã®éƒ½åˆä¸Šdictã§è¡¨ç¾ã•ã‚ŒãŸé…åˆ—ã‚’listã«å¤‰æ›ã™ã‚‹
        choices = []
        print(self.response)
        for ci, cvalue in sorted(self.response["choices"].items(), key=lambda x:x[0]):
            tool_calls = []
            for ti, tvalue in sorted(cvalue["tool_calls"].items(), key=lambda x:x[0]):
                tool_calls.append(tvalue)
            cvalue["tool_calls"] = tool_calls
            choices.append(cvalue)
        self.response["choices"] = choices

def get_assistant(client, mode):
    # IF: https://platform.openai.com/docs/assistants/how-it-works/creating-assistants
    current_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S+00:00")

    if mode == "development":
        instructions=f"ã‚ãªãŸã¯æ±ç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›ç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html]
    else:
        instructions=f"ã‚ãªãŸã¯æ±ç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›ç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, internetAccess.html]

    name=f"æ±ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ({mode})"
    assistant = None
    assistants = client.beta.assistants.list(order='desc', limit="100")
    print(assistants)
    for i in assistants.data:
        if i.created_at < time.time() - 86400:
            client.beta.assistants.delete(assistant_id=i.id)
            time.sleep(.2)
        elif i.name == name:
            assistant = i

    if not assistant:
        assistant = client.beta.assistants.create(
            name=name,
            model="gpt-4o"
        )

    client.beta.assistants.update(
        assistant_id=assistant.id,
        instructions=instructions,
        tools=tools
    )

    return assistant.id

# åˆæœŸåŒ–
if "clients" not in st.session_state:
    st.session_state.clients = {
        "openai": AzureOpenAI(
            azure_endpoint = os.getenv("ENDPOINT_URL"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2024-12-01-preview"
        ),
        "deepseek": ChatCompletionsClient(
            endpoint=os.getenv("DEEPSEEK_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("DEEPSEEK_AZURE_INFERENCE_CREDENTIAL"))
        ),
        "phi4": ChatCompletionsClient(
            endpoint=os.getenv("PHI_4_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("PHI_4_AZURE_INFERENCE_CREDENTIAL"))
        )
    }

if "assistants" not in st.session_state:
    st.session_state.assistants = {
        "gpt-4o": get_assistant(st.session_state.clients["openai"], os.getenv("IASA_DEPLOYMENT_MODE", "development"))
    }

if "conversation" not in st.session_state:
    st.session_state.conversation = ConversationManager(st.session_state.clients, st.session_state.assistants)
conversation = st.session_state.conversation 

models = {
  "o3-mini": {
    "model": "o3-mini",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": False,
    "support_tools": True,
    "support_reasoning_effort": True,
    "streaming": True,
    "pricing": {"in": 1.1, "out":4.4}
  },
  "GPT-4o": {
    "model": "gpt-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "assistant",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": False,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
#  "GPT-4o-nostream": {
#    "model": "gpt-4o",
#    "client": st.session_state.clients["openai"],
#    "api_mode": "assistant",
#    "assistant_id": st.session_state.assistants["gpt-4o"],
#    "support_vision": False,
#    "support_tools": True,
#    "streaming": False,
#    "pricing": {"in": 2.5, "out":10}
#  },
  "GPT-4o-completion": {
    "model": "gpt-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
  "o1": {
    "model": "o1",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "streaming": False,
    "pricing": {"in": 15, "out":60}
  },
  "DeepSeek R1": {
    "model": "DeepSeek-R1",
    "client": st.session_state.clients["deepseek"],
    "api_mode": "inference",
    "streaming": True,
    "pricing": {"in": 0, "out":0}
  },
  "Microsoft Phi-4": {
    "model": "Phi-4",
    "client": st.session_state.clients["phi4"],
    "api_mode": "inference",
    "streaming": True,
    # ã“ã‚Œã¯EastUS2ã®ä¾¡æ ¼ã§ã‚ã‚Šã€æ­£ç¢ºã§ã¯ãªã„
    # https://techcommunity.microsoft.com/blog/machinelearningblog/affordable-innovation-unveiling-the-pricing-of-phi-3-slms-on-models-as-a-service/4156495
    "pricing": {"in": 0.125, "out": 0.5}
  }
}

if "fileCache" not in st.session_state:
    st.session_state.fileCache = {}
if "processing" not in st.session_state:
    st.session_state.processing = False
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = 0
if 'switches' not in st.session_state:
    st.session_state.switches = {
        "code_interpreter": True,
        "file_search": True,
        "get_google_results": True,
        "parse_html_content": True
    }

# ãƒ¡ã‚¤ãƒ³UI
st.title("Dual API Chat Interface")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    model = models[st.selectbox(
        "Model",
        models.keys()
    )]
    options = {}

    st.text("API Mode: " + model["api_mode"])
    st.text("Streaming: " + ("True" if model.get("streaming", False) else "False"))
    st.text("Support vision: " + ("True" if model.get("support_vision", False) else "False"))

    if model.get("support_reasoning_effort", False):
        options["reasoning_effort"] = st.selectbox(
            "reasoning_effort",
            ["low", "medium", "high"],
            index = 2
        )

    uploaded_files = None
    image_files = None
    if model["api_mode"] == "assistant":
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            accept_multiple_files=True,
            key = f"file_uploader_{st.session_state.uploader_key}"
        )
        tool_for_files = st.selectbox(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”¨é€”",
            ["file_search", "code_interpreter"]
        )
        st.session_state.switches[tool_for_files] = True

    if model.get("support_vision", False):
        image_files = st.file_uploader(
            "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type = ["png", "jpeg", "jpg", "webp", "gif"],
            key = f"image_uploader_{st.session_state.uploader_key}"
        )
        detail_level = st.selectbox(
            "ç”»åƒã®è©³ç´°ãƒ¬ãƒ™ãƒ«",
            ["auto", "high", "low"]
        )

    # "code_interpreter", "file_search"ã‚’ä½¿ãˆã‚‹ã®ã¯assistantã®æ™‚ã ã‘
    if model["api_mode"] != "assistant":
        tools = [tool for tool in tools if tool.get("type", None) == "function"]

    # è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
    tool_names = [
        t.get("type") if t.get("type") != "function"
        else t["function"]["name"]
        for t in tools
    ]

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«toolé¸æŠç”¨ãƒˆã‚°ãƒ«ã‚¹ã‚¤ãƒƒãƒã‚’è¡¨ç¤º
    if model.get("support_tools", False):
        st.header("ãƒ„ãƒ¼ãƒ«é¸æŠ")
        switches = {
            name: st.toggle(
                name,
                value=st.session_state.switches[name] if name in st.session_state.switches else False,
                key=f"tool_switch_{name}"
            )
            for name in tool_names
        }
        st.session_state.switches = st.session_state.switches | switches

        # é¸æŠã«åˆã‚ã›ãƒ„ãƒ¼ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        selected_tools = [
            tool
            for i, tool in enumerate(tools)
            if st.session_state.switches[tool_names[i]]
        ]
    else:
        selected_tools = []
    print(selected_tools)

if content := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›"):
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        attachments = []
        if uploaded_files:
            st.toast("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            attachments = conversation.create_attachments_for_assistant(uploaded_files, tool_for_files)
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢

        if image_files:
            st.toast("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]
            for file in image_files:
                # ImageURLãŒã€ImageFileã®ä¸¡æ–¹ä½œæˆã—ã¦ãŠãã€APIå®Ÿè¡Œæ™‚ã«ä¸è¦ãªæ–¹ã¯æ¨ã¦ã‚‹
                # ãŸã ã—ã€Threadæœªä½œæˆæ™‚ã«ã¯ImageURLã®ã¿ä½œæˆã¨ã—ã€Threadä½œæˆæ™‚ã«ImageURLã‹ã‚‰å¤‰æ›ç”Ÿæˆã™ã‚‹
                # ã¨ã„ã†è¨ˆç”»ã ã£ãŸãŒã€ã©ã†ã‚„ã‚‰gpt-4oã¯Assistant APIæ™‚ã«ã¯ç”»åƒã‚’èªè­˜ã§ããªã„æ¨¡æ§˜ï¼ˆæœ¬äººè«‡ï¼‰
                # ç¾çŠ¶ã€Assistant API, Visionä¸¡å¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ãŒä»–ã«ãªã„ã®ã§ã€ä¸€æ—¦ã‚ãã‚‰ã‚ã€ImageURLã®ã¿å¯¾å¿œã€‚
                content.append(conversation.create_ImageURLContentBlock(file,detail_level))
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
    
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        conversation.add_message(model, "user", content, attachments)
        # å‡¦ç†ä¸­ã¸ç§»è¡Œ
        st.session_state.processing = True
        st.rerun()  # ã“ã“ã§ä¸€æ—¦å†æç”»(ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªã‚¢ãªã©ã«å¿…è¦)
    
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
pretty_print(conversation.thread.messages)

if st.session_state.get("processing"):
    # å‡¦ç†ã™ã¹ããƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹

    # APIå®Ÿè¡Œ
    try:
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        with st.chat_message("assistant"):

            content, metadata = execute_api(
                model,
                selected_tools,
                conversation,
                options
                )
        
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å±¥æ­´ã«è¿½åŠ 
            conversation.add_message(model, "assistant", content, None, metadata)
            st.session_state.processing = False

            # æœ€çµ‚çš„ãªå†…å®¹ã§æç”»ã—ãªãŠã™ã¹ãã‹ï¼Ÿå½“é¢ä¸è¦ã¨åˆ¤æ–­ã€‚
            # placeholderã‚’ä½¿ã£ã¦æ¸…æ›¸ã™ã‚‹äº‹ã‚‚è€ƒãˆãŸãŒã€ãƒ©ã‚¦ã‚¶å´ã¨ã®åŒæœŸã«å¤±æ•—ã—ã€å‰ã®ã‚³ãƒ³ãƒ†ãƒŠã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒ
            # æ®‹ã£ã¦ã—ã¾ã„ã€ã“ã‚Œã¯ã†ã¾ãã„ã‹ãªã„ã€‚
            # streamlitã§ã¯æ—¢ã«æç”»ã—ãŸã‚‚ã®ã‚’å¤‰æ›´ã™ã‚‹ã®ã¯ã‚„ã‚ãŸæ–¹ãŒã„ã„ã‹ã‚‚ã€‚
            # ã©ã†ã—ã¦ã‚‚å†æç”»ãŒå¿…è¦ãªã‚‰(å¼•ç”¨ç•ªå·ã®æ›¸ãæ›ãˆãªã©)ã€ç´ ç›´ã«rerun()ã—ãŸæ–¹ãŒã„ã„ã€‚
#        st.rerun()
    
    except Exception as e:
        st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.exception(e)
        st.button("ãƒªãƒˆãƒ©ã‚¤")
