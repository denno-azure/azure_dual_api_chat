import os
from os.path import dirname, join
from dotenv import load_dotenv
from io import BytesIO
import traceback
import streamlit as st
import openai
import time
from datetime import datetime, timezone
import json
import time
import base64
import re
from functools import reduce
from mimetypes import guess_type
# from collections import defaultdict
from openai import AssistantEventHandler, AzureOpenAI
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from typing_extensions import override
from dataclasses import dataclass, field
from typing import Any, List, Dict, Optional, Union
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessage,
    ChatCompletionChunk
)
# from openai.types.chat.chat_completion import ChatCompletion
# from openai.types.chat.chat_completion_message import ChatCompletionMessage
# from openai.types.chat.chat_completion_chunk import ChatCompletionChunk
from openai.types.beta.threads import (
    TextContentBlock,
    ImageURLContentBlock,
    ImageFileContentBlock,
    ImageFile,
    ImageURL,
    Text,
    Annotation,
    Run
)
# from openai.types.beta.threads.image_file import ImageFile
# from openai.types.beta.threads.image_url import ImageURL
# from openai.types.beta.threads.text import Text
# from openai.types.beta.threads.annotation import Annotation
import concurrent.futures
import customTools
import serperTools
import internetAccess

@dataclass
class GPTHallucinatedFunctionCall:
    tool_uses: List['HallucinatedToolCalls']
    def __post_init__(self):
        self.tool_uses = [HallucinatedToolCalls(**i) for i in self.tool_uses]

@dataclass
class HallucinatedToolCalls:
    recipient_name: str
    parameters: dict

#@dataclass
#class ChatCompletionMessagea:
#    tool_calls: List['ChatCompletionMessageToolCall']
#    role: str = "assistant"
#    content: str = None
#    def __post_init__(self):
#        self.tool_calls = [ChatCompletionMessageToolCall(**i) for i in self.tool_calls]

#@dataclass
#class ChatCompletionMessageToolCall:
#    id: str
#    function: 'ToolFunction'
#    type: str
#    def __post_init__(self):
#        self.function = ToolFunction(**self.function)

#@dataclass
#class ToolFunction:
#    name: str
#    arguments: str

dotenv_path = join(dirname(__file__), ".env.local")
load_dotenv(dotenv_path)

tools=[{"type": "code_interpreter"}, {"type": "file_search"}, customTools.time, serperTools.run, serperTools.results, serperTools.scholar, serperTools.news, serperTools.places, internetAccess.html]

# response_placeholderå†…ã«ã¯è¤‡æ•°ã®è¦ç´ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰ãŒå…¥ã‚‹
# response_placeholderã¯st.empty()ã‚’å‰æ
# å¾Œã‹ã‚‰å†…å®¹ã‚’ä¸¸ã”ã¨å·®ã—æ›¿ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã€ä¸­ã«st.containerã‚’ä½œã£ã¦ã‹ã‚‰é…ç½®ã™ã‚‹
# self.placeholderã¯ç¾åœ¨æ›¸ãè¾¼ã¿ä¸­ã®TextContentBlockã«ç›¸å½“ã™ã‚‹st.empty()
#   parent_streamç„¡ã—ã®å ´åˆã¯st.containerã®ä¸­ã«st.empty()ã‚’é…ç½®
#   parent_streamã‚ã‚Šã®å ´åˆã¯è¦ªã®placeholderã‚’å¼•æ•°response_placeholderã§å—ã‘å–ã£ã¦ç”¨ã„ã‚‹
class StreamHandler(AssistantEventHandler):
    @override
    def __init__(self, client):
#    def __init__(self, client, parent_stream = None):
        super().__init__()
        self.client = client
#        self.container = parent_stream.container if parent_stream else response_placeholder.container(key=f"streamContainer{time.time()}")
#        self.placeholder = response_placeholder if parent_stream else self.container.empty()
#        self.placeholder = parent_stream.placeholder if parent_stream else None
#        # è¦ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰èµ·å‹•ã•ã‚ŒãŸå ´åˆã«è¦ªã®å€¤ã‚’å¼•ãç¶™ãï¼ˆè¦ªå­ã¯ä¸¦åˆ—å‹•ä½œã—ãªã„å‰æï¼‰
#        self.full_response = parent_stream.full_response if parent_stream else ""
        self.content = []
#        self.current_delta = parent_stream.current_delta if parent_stream else ""
#        self.last_update = time.time()
        # è¦ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰èµ·å‹•ã•ã‚ŒãŸå ´åˆã«è¦ªã‚’ä¿æŒï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†æ™‚ã«è¦ªã«å€¤ã‚’è¿”ã™éš›ã«ä½¿ç”¨ï¼‰
#        self.parent_stream = parent_stream
        self.final_run = None

    @override
    def on_event(self, event):
      # Retrieve events that are denoted with 'requires_action'
      # since these will have our tool_calls
      if event.event == 'thread.run.requires_action':
          run_id = event.data.id  # Retrieve the run ID from the event data
          self.handle_requires_action(event.data, run_id)
#      elif event.event == 'thread.run.completed':
#          self.current_run = event.data

#    @override
#    def on_text_created(self, text: Any) -> None:
#        self.placeholder = response_placeholder.empty()
#        self.current_delta = ""

#    @override
#    def on_text_delta(self, delta: Any, snapshot: Any) -> None:
        # é€šå¸¸APIã‹ã‚‰èµ·å‹•ã•ã‚Œã‚‹ãŒã€function callingã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹éš›ã«ã‚‚ä½¿ãˆã‚‹ã‚ˆã†deltaãŒdictã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã—ãŸ
#        value = delta["value"] if isinstance(delta, dict) else delta.value
#        if isinstance(value, str):
#            self.current_delta += value
#        now = time.time()
#        # 0.1ç§’ã”ã¨ã«æ›´æ–°
#        if now - self.last_update > 0.1:
#            self.flush_updates()
#            self.last_update = now

    @override
    def on_image_file_done(self, image_file: ImageFile) -> None:
        print("on_image_file_done ImageFile:", image_file)
        self.content.append(ImageFileContentBlock(type="image_file", image_file=image_file))
        st.image(get_file(image_file.file_id))
#        response_placeholder.image(get_file(image_file.file_id))
#        image_bytes = get_file(image_file.file_id)
#        image_encoded = base64.b64encode(image_bytes).decode()
#        self.current_delta += f'<img src="data:image/png;base64,{image_encoded}" />'

    @override
    def on_text_done(self, text: Text) -> None:
        print("on_text_done Text:", text)
        self.content.append(TextContentBlock(type="text", text=text))
        value, files = parse_annotations(text.value, text.annotations)
#        self.full_response = value
#        self.current_delta = ""
#        self.flush_updates(final=True)
#        if files:
#            st.caption(f"ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
        put_buttons(files, "stream")

#    def flush_updates(self, final=False):
##        if self.current_delta:
#        self.full_response += self.current_delta
#        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ›´æ–°
##        self.placeholder.markdown(
##            self.full_response + ("" if final else "â–Œ"),
##            unsafe_allow_html=True
##        )
#        self.current_delta = ""

    @override
    def on_tool_call_created(self, tool_call: Any) -> None:
        print(f"\nassistant > tool_call_created > {tool_call.type}\n", flush=True)
        if tool_call.type != "function":
            st.toast(tool_call.type)
        print(tool_call, flush=True)

    @override
    def on_tool_call_delta(self, delta: Any, snapshot: Any) -> None:
        if delta.type == "code_interpreter":
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="", flush=True)
            if delta.code_interpreter.outputs:
                print("\n\noutput >", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        print(f"\n{output.logs}", flush=True)

    def handle_requires_action(self, data, run_id):
#        if not self.placeholder:
#            self.placeholder = response_placeholder.empty()
        print(f"\nassistant > {data}\n", flush=True)
        tool_calls = data.required_action.submit_tool_outputs.tool_calls

      # ä¸¦åˆ—å‡¦ç†ãŒå¿…è¦ãªå ´åˆ
#      if is_parallel_execution(tool_calls):
        tool_outputs = handle_tool_calls(tool_calls)
#      else:
          # é€šå¸¸ã®ç›´åˆ—å‡¦ç†
#          tool_outputs = []

#          for tool in tool_calls:
#              fname = tool.function.name
#              fargs = json.loads(tool.function.arguments)
#              print(f"Function call: {fname}")
#              print(f"Function arguments: {fargs}")
#
#              fresponse = function_calling(fname, fargs, self)
#
#              tool_outputs.append({
#                  "tool_call_id": tool.id,
#                  "output": fresponse,
#              })

        # Submit all tool_outputs at the same time
#        self.submit_tool_outputs(tool_outputs, run_id)
#
#    def submit_tool_outputs(self, tool_outputs, run_id):
#        # Use the submit_tool_outputs_stream helper
        with self.client.beta.threads.runs.submit_tool_outputs_stream(
            thread_id=self.current_run.thread_id,
            run_id=self.current_run.id,
            tool_outputs=tool_outputs,
            event_handler=StreamHandler(self.client) # event_handler = selfã‚’æŒ‡å®šã™ã‚‹ã¨ã€åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯å‡ºæ¥ãªã„æ—¨ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã€‚toolå›žç­”ã”ã¨ã«å­StreamHandlerã‚’ç”Ÿæˆã™ã‚‹ã—ã‹ãªã„ã€‚ã“ã‚ŒãŒæ­£ã—ã„ã‚„ã‚Šæ–¹ãªã®ã‹ã¯ä¸æ˜Žã€‚
        ) as stream:
            st.write_stream(stream.text_deltas)
            stream.until_done()
            # AssistantEventHandlerã«çµ„ã¿è¾¼ã¿ã®ã‚¤ãƒ™ãƒ³ãƒˆæ©Ÿæ§‹ã«ã‚ˆã‚Šã€thread.run.completed, canceled, 
            # expired, failed, required_action, incompleteã®éš›ã«ã€__current_runãŒæ›´æ–°ã•ã‚Œã€
            # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£current_run()ã«ã‚ˆã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹
        self.final_run = stream.final_run or stream.current_run
        self.content += stream.content

#    @override
#    def on_message_done(self, message):
#        # è¦ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ç¾åœ¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
#        if self.parent_stream:
##            self.parent_stream.full_response = self.full_response
#            self.parent_stream.content += self.content
##            self.parent_stream.placeholder = self.placeholder
##            self.parent_stream.current_delta = self.current_delta

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¿æŒã¨ã€UIã¸ã®è¡¨ç¤ºã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã€‚function callingã®å‘¼ã³å‡ºã—å…ˆã§è¡¨ç¤ºã‚’è¡Œã†ãŸã‚ã«ä½¿ç”¨
# ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‰æã¨ã—ã€placeholderã«ä¸€ã¤ã®markdownãŒé…ç½®ã•ã‚Œã‚‹
#class ResponseHandler:
#    def __init__(self, response_placeholder, initial_response = ""):
#        self.full_response = initial_response
#        self.placeholder = response_placeholder.empty()
#
#    def on_text_delta(self, delta, snapshot = None):
#        self.full_response += delta["value"] if isinstance(delta, dict) else delta.value
#        self.placeholder.markdown(self.full_response + "â–Œ")

# function callingã®ä¸¦åˆ—å®Ÿè¡Œä¸­ã¯streamlitã«ã‚ˆã‚‹UIå‡ºåŠ›ã¯å‡ºæ¥ãªã„ã®ã§ãƒ€ãƒŸãƒ¼ã®å‡ºåŠ›å…ˆã¨ã—ã¦ä½¿ç”¨
#class DummyHandler:
#    def __init__(self):
#        return
#
#    def on_text_delta(self, delta, snapshot = None):
#        return

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®å®šç¾©
@dataclass
class ChatMessage:
    role: str
    content: List[Union[ImageFileContentBlock, ImageURLContentBlock, TextContentBlock]] # Assistant APIã®contentå®šç¾©
    files: List[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)
#    def __init__(self, role, content, files=None, metadata={}):
#        self.role = role
#        self.content = content # Assistant APIã®contentå®šç¾©ã‚’ä½¿ç”¨: TextContentBlock, ImageFileContentBlockç­‰ã®ãƒªã‚¹ãƒˆ
#        self.files = files or []
#        self.metadata = {}

# ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹
class ChatThread:
    def __init__(self, client):
        self.client = client
        self.messages = []
        self.thread_id = None

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        if isinstance(content, str):
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]

        self.messages.append(ChatMessage(role, content, files, metadata))

# gpt-4oã®Assistant APIã§ç”»åƒèªè­˜ãŒå‡ºæ¥ãªã„å•é¡Œã¯ã¨ã‚Šã‚ãˆãšãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨ã™ã‚‹
#        # Assistant APIã¯ImageURLContentBlockã‚’èªè­˜ã—ãªã„ï¼Ÿã™ã‚‹ã¯ãšã ãŒãƒ»ãƒ»
#        # å½“é¢ImageFileã§ä¸Žãˆã‚‹ã“ã¨ã¨ã—ã€é‡è¤‡ã™ã‚‹ImageURLã¯å–ã‚Šé™¤ã
#        content = [cont for cont in content if not isinstance(cont, ImageURLContentBlock)]
        # Assistant APIã‚’æœªä½¿ç”¨ã®æ®µéšŽã§ã¯thread_idã¯å­˜åœ¨ã—ãªã„ã€‚åˆã‚ã¦ä½¿ã†æ™‚ã«ä½œæˆã—ã¦éŽåŽ»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        # Assistant APIæ™‚ã«ã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯è‡ªå‹•çš„ã«Threadã«è¨˜éŒ²ã•ã‚Œã‚‹ã€‚
        if self.thread_id and (model["api_mode"] != "assistant" or role != "assistant"):
            self.client.beta.threads.messages.create(
                thread_id = self.thread_id,
                role = role,
                content = self.content_to_content_param(content),
                attachments = files
            )
    def get_last_message(self):
        return self.messages[-1]

    def get_thread_id(self):
        """
        Assistant APIç”¨ã®thread_idã‚’è¿”ã™ã€‚åˆã‚ã¦Assistant APIã‚’ä½¿ã†ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§
        threadã‚’ä½œæˆã—ã€éŽåŽ»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹
        """
        if self.thread_id:
            return self.thread_id

        thread = self.client.beta.threads.create(
            messages = [
                {
                    "role": msg.role,
                    "content": self.content_to_content_param(msg.content),
                    "attachments": msg.files
                }
                for msg in self.messages
            ]
        )
        self.thread_id = thread.id
        
        return self.thread_id

    @staticmethod
    def content_to_content_param(content: List[Union[ImageFileContentBlock, ImageURLContentBlock, TextContentBlock]]) -> List[dict]:
        content_param = []
        for block in content:
            if block.type == "text":
                content_param.append({
                    "type": block.type,
                    "text": block.text.value
                })
            elif block.type == "image_file":
                content_param.append({
                    "type": block.type,
                    "image_file": {"file_id": block.image_file.file_id, "detail": block.image_file.detail},
                })
            elif block.type == "image_url":
                content_param.append({
                    "type": block.type,
                    "image_url": {"url": block.image_url.url, "detail": block.image_url.detail},
                })
            else:
                raise ValueError(f"æœªçŸ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã® type: {block.type}")
        return content_param

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹
class ConversationManager:
    def __init__(self, clients, assistants):
        self.client = clients["openai"]
        self.thread = ChatThread(self.client)
#        self.deepseek_client = clients["deepseek"]
        self.assistants = assistants

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ChatThreadã«è¿½åŠ """
        self.thread.add_message(model, role, content, files, metadata)

    def get_completion_messages(self, model, text_only=False):
        """Completion APIç”¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰æ›"""
        messages = []
        for msg in self.thread.messages:
            # Assistant APIç”¨ã®ImageFileContentBlockã¯é™¤ã
            content = [cont for cont in msg.content if not isinstance(cont, ImageFileContentBlock)]

            # Visionã‚µãƒãƒ¼ãƒˆã®ç„¡ã„ãƒ¢ãƒ‡ãƒ«ã«Imageã‚’ä¸Žãˆã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§é™¤ã
            if not model.get("support_vision", False):
                content = [cont for cont in content if isinstance(cont, TextContentBlock)]

            if text_only:
                # Deepseekãªã©ã€ãƒ†ã‚­ã‚¹ãƒˆã ã‘å¿…è¦ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
                content = "\n".join([cont.text.value for cont in content if isinstance(cont, TextContentBlock)])
            else:
                # ãã†ã§ãªã„å ´åˆã¯classã‹ã‚‰dictã«å¤‰æ›ã™ã‚‹
                content = self.thread.content_to_content_param(content)

            messages.append({
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": content
            })
        return messages

    def create_attachments_for_assistant(self, files, tool_for_files):
        """Assistantç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        attachments = []
        for file in files:
            file.seek(0)
            response = self.client.files.create(
                file=file,
                purpose="assistants"
            )
            attachments.append(
                    {
                        "file_id": response.id,
                        "tools": [{"type": tool_for_files}],
                    }
            )
        return attachments

    def create_ImageURLContentBlock(self, file, detail_level):
        mime_type = guess_type(file.name)[0]
        image_encoded = base64.b64encode(file.getvalue()).decode()
        image_url = f'data:{mime_type};base64,{image_encoded}'
        return ImageURLContentBlock(
            type="image_url",
            image_url=ImageURL(url=image_url, detail=detail_level)
        )

# gpt-4oãŒAssistant APIã§ç”»åƒã‚’èªè­˜ã—ãªã„ã®ã§ã€ImageFileãªã‚‰ã¨æ€ã£ã¦åŠ ãˆãŸãŒã€ã©ã†ã‚„ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ–¹ã®å•é¡Œã‚‰ã—ã„
#    def create_ImageFileContentBlock(self, file, detail_level):
#        response = self.client.files.create(
#            file=file,
#            purpose="vision"
#            # "vision"ã‚’æŒ‡å®šã™ã‚‹ã¨ã€"purpose contains an invalid purpose vision"ã¨è¨€ã‚ã‚Œã¦ã—ã¾ã†ã€‚
#            # Azureã®APIãŒè¿½ã„ã¤ã„ã¦ã„ãªã„å¯èƒ½æ€§ã‚ã‚Šã€‚"assistant"ãªã‚‰å—ã‘ä»˜ã‘ã‚‹ãŒã€gpt-4oã¯è‡ªåˆ†ã«ã¯ç”»åƒèªè­˜èƒ½åŠ›ãŒç„¡ã„ã¨è¨€ã†
#        )
#        return ImageFileContentBlock(
#            type="image_file",
#            image_file=ImageFile(file_id=response.id, detail=detail_level)
#        )

def pretty_print(messages: List[ChatMessage]) -> None:
    i = -1
    m = None
    for i0, m0 in enumerate(messages):
#        print("role:", m.role)
#        print("content:", m.content)
        if i != -1:
            with st.chat_message("assistant" if m.role == "assistant" else "user"):
                pretty_print_message(i, m)
        i = i0
        m = m0

    if i != -1:
        with st.chat_message("assistant" if m.role == "assistant" else "user"):
            pretty_print_message(i, m, with_token_summery=True)


def pretty_print_message(key, message, with_token_summery=False):
    for j, cont in enumerate(message.content):
        if isinstance(cont, ImageFileContentBlock) and message.role == "assistant":
            # è‡ªåˆ†ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã®ã§ã€asssistantã®å ´åˆã®ã¿è¡¨ç¤º
            st.image(get_file(cont.image_file.file_id))
        if isinstance(cont, ImageURLContentBlock):
            st.image(cont.image_url.url)
        if isinstance(cont, TextContentBlock):
            value, files = parse_annotations(cont.text.value, cont.text.annotations)
            st.markdown(value, unsafe_allow_html=True)
#            if files:
#                st.caption(f"ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
            put_buttons(files, f"hist{key}-{j}")
    print(message)
    print(with_token_summery)
    if "file_search_results" in message.metadata:
        put_quotations(message.content, message.metadata["file_search_results"])
    if with_token_summery and "token_summery" in message.metadata:
        st.markdown(message.metadata["token_summery"])

def put_buttons(files, key=None) -> None:
    for i, file in enumerate(files):
        if key:
            key=f"{key}-{i}"
        else:
            key = None
        if file["type"] == "file_path":
            st.download_button(
                f"{file["index"]}: {file["filename"]} : ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                get_file(file["file_id"]),
                file_name=file["filename"],
                key=key
            )
#        else:
#            st.write(f"{file["text"]}: {file["filename"]}")

def put_quotations(content, file_search_results):
    citations = {}
    for cont in content:
        if isinstance(cont, TextContentBlock):
            for annotation in cont.text.annotations:
                if annotation.type == "file_citation" and (match := re.search(r'(\d+):(\d+)', annotation.text)):
                    i = int(match[2])
                    if i not in citations:
                        citations[i] = annotation.text
    for i, text in citations.items():
        result = file_search_results[i]
        with st.expander(f"{text}: {result.file_name}"):
            st.write(f"""
~~~
score: {result.score}
~~~
{result.content[0].text}
""")
            # fileId: {result.file_id}

def get_file(file_id: str) -> bytes:
    key = f"content_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openai"]
    retrieve_file = client.files.with_raw_response.content(file_id)
    content: bytes = retrieve_file.content
    st.session_state.fileCache[key] = content
    return content

def get_file_info(file_id: str) -> bytes:
    key = f"info_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openai"]
    retrieve_file = client.files.retrieve(file_id)
    st.session_state.fileCache[key] = retrieve_file
    return retrieve_file

def parse_annotations(value: str, annotations: List[Annotation]):
    files = []
    print(value)
    print(annotations)
    for (
        index,
        annotation,
    ) in enumerate(annotations):
#        value = value.replace(
#            annotation.text,
#            f" [{index}]",
#        )
        # FilePathAnnotation
        if annotation.type == "file_path":
            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_path.file_id,
                    "filename": annotation.text.split("/")[-1],
                    "text": annotation.text,
                    "index": index
                }
            )
        elif annotation.type == "file_citation":
            info = get_file_info(annotation.file_citation.file_id)
            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_citation.file_id,
                    "filename": info.filename,
                    "text": annotation.text,
                    "index": index
                }
            )
    value = re.sub(r'\[([^\]]*)\]\((sandbox:[^)]*)\)', r'ãƒœã‚¿ãƒ³\1', value)
    return value, files

#def is_parallel_execution(tool_calls: List[Dict]) -> bool:
#    """
#    ä¸¦åˆ—å®Ÿè¡ŒãŒå¿…è¦ã‹åˆ¤å®šã™ã‚‹é–¢æ•°
#    ï¼ˆä¾‹: ç‰¹å®šã®ãƒ„ãƒ¼ãƒ«åã‚’å«ã‚€å ´åˆã‚„æ˜Žç¤ºçš„ãªãƒ•ãƒ©ã‚°ãŒã‚ã‚‹å ´åˆï¼‰
#    """
#    return any(tool.function.name == "multi_tool_use.parallel" for tool in tool_calls)

def handle_tool_calls(tool_calls: List[Dict], mode = "assistant") -> List[Dict]:
    """
    ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
    """
    print(tool_calls)
    def add_output(outputs, tool_call_id, output, mode, fname = None):
        if mode == "assistant":
            # Assistant APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "output": output
            })
        else:
            # Completion APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "role": "tool",
                "name": fname,
                "content": output,
            })

    tool_outputs = []

    for tool in tool_calls:
        function = tool.function
#        function.name = "multi_tool_use.parallel"
        fname = function.name
#        function.arguments = '{"tool_uses": [{"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 processor"}}, {"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 chip"}}]}'
        fargs = json.loads(function.arguments)
        print(f"Function call: {fname}")
        print(f"Function arguments: {fargs}")

        if function.name == "multi_tool_use.parallel":
            # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼šAIãŒä¸¦åˆ—å®Ÿè¡Œã‚’è¦æ±‚ã—ã¦ãã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
            # ä»•æ§˜å¤–ã®å‹•ä½œ(ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³)ã¨ã„ã†èª¬ã‚‚ã‚ã‚‹ã€‚å‹•ä½œæ¤œè¨¼æœªæ¸ˆã€‚
            # We need to deserialize the arguments
            caught_calls = GPTHallucinatedFunctionCall(**(json.loads(function.arguments)))
            tool_uses = caught_calls.tool_uses

            # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
            with concurrent.futures.ThreadPoolExecutor() as executor:
#                dummyHandler = DummyHandler()

                # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œç”¨ã‚¿ã‚¹ã‚¯ã«å¤‰æ›
                future_to_tool = {
                    executor.submit(
                        function_calling,
                        tool_use.recipient_name.rsplit('.', 1)[-1],
                        tool_use.parameters
                    ): {"id": tool.id, "fname": tool_use.recipient_name.rsplit('.', 1)[-1]}
                    for tool_use in tool_uses
                }

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæžœã‚’åŽé›†
                results = []
                for future in concurrent.futures.as_completed(future_to_tool):
                    tool_call_id = future_to_tool[future]["id"]
                    fname = future_to_tool[future]["fname"]
                    print(f"fname: {fname}")
                    print(f"tool_call_id: {tool_call_id}")
                    try:
                        results.append(future.result())
                    except Exception as e:
                        print(f"Error: {str(e)}")
                        results.append(json.dumps(f"Error: {str(e)}"))

                print(results)
                # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«ã¤ã„ã¦ã€tool_call_idã¯ä¸€ã¤ã—ã‹ãªãã€jsonã‚’æ”¹è¡Œã§é€£çµã—ä¸€ã¤ã®tool_outputã«ã¾ã¨ã‚ã¦è¿”ã™ã€‚ã“ã®æƒ…å ±ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä¼šè©±ã‚ˆã‚Šå¾—ãŸãŒæ­£ã—ã„ã‹åˆ†ã‹ã‚‰ãªã„ã€‚
                add_output(tool_outputs, tool_call_id, "\n".join(results), mode, fname)
        else:
              # é †æ¬¡ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
              fresponse = function_calling(fname, fargs)
              add_output(tool_outputs, tool.id, fresponse, mode, fname)

    print("tool_outputs:")
    print(tool_outputs)
    return tool_outputs

def function_calling(fname, fargs):
        print(f"fc fname: {fname}")
        print(f"fc fargs: {fargs}")
        if fname == "get_current_weather":
            fresponse = customTools.get_current_weather(
                location=fargs.get("location"),
            )
        elif fname == "get_current_datetime":
            st.toast("[datetime]", icon="ðŸ•’");
#            streamHandler.on_text_delta({"value": ":blue-background[datetime]"}, None);
            fresponse = customTools.get_current_datetime(
                timezone=fargs.get("timezone")
            )
        elif fname == "get_google_serper":
            st.toast(f"[Google Serper] {fargs.get("query")}", icon="ðŸ”");
#            streamHandler.on_text_delta({"value": ":blue-background[google]"}, None);
            fresponse = serperTools.get_google_serper(
                query=fargs.get("query")
            )
        elif fname == "get_google_results":
            st.toast(f"[Google detail] {fargs.get("query")}", icon="ðŸ”");
#            streamHandler.on_text_delta({"value": ":blue-background[google detail]"}, None);
            fresponse = serperTools.get_google_results(
                query=fargs.get("query")
            )
        elif fname == "get_google_scholar":
            st.toast(f"[Google scholar] {fargs.get("query")}", icon="ðŸŽ“");
#            streamHandler.on_text_delta({"value": ":blue-background[google scholar]"}, None);
            fresponse = serperTools.get_google_scholar(
                query=fargs.get("query")
            )
        elif fname == "get_google_news":
            st.toast(f"[Google news] {fargs.get("query")}", icon="ðŸ“°");
#            streamHandler.on_text_delta({"value": ":blue-background[google news]"}, None);
            fresponse = serperTools.get_google_news(
                query=fargs.get("query")
            )
        elif fname == "get_google_places":
            st.toast(f"[Google places] {fargs.get("query")}", icon="ðŸ½ï¸");
#            streamHandler.on_text_delta({"value": ":blue-background[google places]"}, None);
            fresponse = serperTools.get_google_places(
                query=fargs.get("query"),
                country=fargs.get("country", "jp"),
                language=fargs.get("language", "ja")
            )
        elif fname == "parse_html_content":
            st.toast("[parse html content]", icon="ðŸ‘€");
#            streamHandler.on_text_delta({"value": ":blue-background[parse html content]"}, None);
            fresponse = internetAccess.parse_html_content(
                url=fargs.get("url"),
                query=fargs.get("query", "headings"),
                heading=fargs.get("heading", None)
            )
        return fresponse

# APIå®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
def execute_api(model, selected_tools, conversation, options = {}):

    thread = conversation.thread
    client = model["client"]

    if model["api_mode"] == "inference":
        # https://learn.microsoft.com/en-us/rest/api/aifoundry/modelinference/
        messages = conversation.get_completion_messages(model, text_only=True)
        print(messages)
        try:
            if model["streaming"]:
#                responseHandler = ResponseHandler(response_placeholder)
                response = client.complete({
                    "stream": True,
                    "messages": messages,
# ç©ºã®toolsã‚’ä¸ŽãˆãŸã ã‘ã§ã‚‚ä¸å®‰å®šã«ãªã‚‹?ã„ã‚„ã€toolsã‚’ä¸Žãˆãªãã¦ã‚‚ã“ã®ã‚¨ãƒ©ãƒ¼ã¯å‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ã‚µãƒ¼ãƒãƒ¼å´ã®æ··é›‘çŠ¶æ³ã«ã‚ˆã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿ DeepSeek APIã‚¨ãƒ©ãƒ¼: Operation returned an invalid status 'Too Many Requests' Content: Please check this guide to understand why this error code might have been returned
#                    "tools": [],
# ç¾æ™‚ç‚¹ã§function callingã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã€‚toolsã‚’æŒ‡å®šã™ã‚‹ã¨ã€åå¿œãŒæ­¢ã¾ã£ã¦ã—ã¾ã†ã€‚2025/2
# https://github.com/deepseek-ai/DeepSeek-R1/issues/9
#                    "tools": [customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html],
#                    "tools": [{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html]
                    "model": model["model"],
                    "max_tokens": 4096
                })
                print(response)
#                full_response = ""
#                for chunk in response:
#                    print(chunk)
#                    if chunk.choices and chunk.choices[0].delta.content:
#                        full_response += chunk.choices[0].delta.content
#                    response_placeholder.markdown(full_response + "â–Œ")
                digester = completion_streaming_digester(response)
                full_response = st.write_stream(digester.generator)
                response = digester.response
#               response = handle_completion_streaming(response, responseHandler)
                response_message = ChatCompletionMessage.model_validate(response["choices"][0])
                print(response)
                full_response = response_message.content
            else:
                response = client.complete({
                    "messages": messages,
                    "max_tokens": 4096
                })
                full_response = response.choices[0].message.content

            token_summery = get_token_summery(response, model)
            st.markdown(token_summery)
#            response_placeholder.markdown(full_response + token_summery)
            metadata = {"token_summery": token_summery}
            return full_response, metadata

        except Exception as e:
            st.error(f"Azure AI Model Inference APIã‚¨ãƒ©ãƒ¼")
            raise

    elif model["api_mode"] == "assistant":
        thread_id = conversation.thread.get_thread_id()
        print(thread_id)

        args = {"thread_id": thread_id, "assistant_id": model["assistant_id"]} | options

        if model["support_tools"]: # selected_toolsãŒç©ºã®å ´åˆã‚‚assistantè¨­å®šã‚’ä¸Šæ›¸ã
            args["tools"] = selected_tools
            print(f"selected tools: {selected_tools}")

        if model["streaming"]:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Assistant APIå®Ÿè¡Œ
            args["event_handler"] = StreamHandler(client)
            try:
                print(args)
                with client.beta.threads.runs.stream(**args) as stream:
                    st.write_stream(stream.text_deltas)
                    stream.until_done()

                run = stream.final_run or stream.current_run
                content = stream.content
                print(content)
                print(run)
                file_search_results = get_file_search_results(thread_id, run.id)
                put_quotations(content, file_search_results)
                token_summery = get_token_summery(run, model)
                st.markdown(token_summery)
#                response_placeholder.markdown(token_summery)
                metadata = {"token_summery": token_summery, "file_search_results": file_search_results}
                return content, metadata

            except Exception as e:
                st.error(f"Assistant(streaming) APIã‚¨ãƒ©ãƒ¼")
                raise

        else:
            # éžã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆ Assistant APIã®å®Ÿè¡Œ
            try:
                # å®Ÿè¡Œé–‹å§‹
                run = client.beta.threads.runs.create(**args)

                # å®Ÿè¡Œå®Œäº†ã‚’å¾…æ©Ÿ
                while run.status not in ["completed", "failed"]:
                    print(run)
                    time.sleep(1)
                    run = client.beta.threads.runs.retrieve(
                        thread_id=thread.thread_id,
                        run_id=run.id
                    )
                    if run.status == "requires_action":
                        print(run.required_action)
                        messages = handle_tool_calls(run.required_action.submit_tool_outputs.tool_calls, "assistant")
                        run = client.beta.threads.runs.submit_tool_outputs(
                          thread_id=thread.thread_id,
                          run_id=run.id,
                          tool_outputs=messages
                        )

                if run.status == "failed":
                    raise Exception(f"å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {run.last_error}")

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
                print(run)
                messages = client.beta.threads.messages.list(
                    thread_id=thread.thread_id,
                    limit=1
                )
                print(messages)
                content = messages.data[0].content
                pretty_print_message("assist_msg", messages.data[0])
                token_summery = get_token_summery(run, model)
                st.markdown(token_summery)
#                response_placeholder.markdown(content[0].text.value + token_summery)
                metadata = {"token_summery": token_summery}
                return content, metadata

            except Exception as e:
                st.error(f"Assistant APIã‚¨ãƒ©ãƒ¼")
                raise

    else:
        # Completion APIå®Ÿè¡Œå‡¦ç†
        messages = conversation.get_completion_messages(model)
        try:
#            responseHandler = ResponseHandler(response_placeholder)
                    
            args = {"model": model["model"], "messages": messages} | options

            print(model)
            if model["streaming"]:
                args["stream"] = True
                args["stream_options"] = {"include_usage": True}

            if model["support_tools"] and selected_tools:
                args["tools"] = selected_tools
                print(f"selected tools: {selected_tools}")

            full_response = ""
            while True:
                print(f"args: {args}")
                response = client.chat.completions.create(**args)
                print(response)

                if model["streaming"]:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    digester = completion_streaming_digester(response)
                    full_response += st.write_stream(digester.generator)
                    response = digester.response
#                    response = handle_completion_streaming(response, responseHandler)
                    print(response)
                    response_message = ChatCompletionMessage.model_validate(response["choices"][0])

                else:
                    # éžã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    response_message = response.choices[0].message
                    if hasattr(response_message, "content") and response_message.content:
                        st.write(response_message.content)
                        full_response += response_message.content
#                        responseHandler.on_text_delta({"value": response_message.content}, None)
                    print(response)

                messages.append(response_message.model_dump())

                if hasattr(response_message, "tool_calls") and response_message.tool_calls:
                    messages += handle_tool_calls(response_message.tool_calls, "completion")

                else:
                    break
                
#            full_response = responseHandler.full_response
            token_summery = get_token_summery(response, model)
            st.markdown(token_summery)
#            response_placeholder.markdown(full_response + token_summery)
            metadata = {"token_summery": token_summery}
            return full_response, metadata

        except Exception as e:
            st.error(f"Completion APIã‚¨ãƒ©ãƒ¼")
            raise

def get_file_search_results(thread_id, run_id):
    client = st.session_state.clients["openai"]

    # Run Stepã‚’å–å¾—
    run_steps = client.beta.threads.runs.steps.list(
        thread_id=thread_id,
        run_id=run_id
    )
#    print(run_steps)

    # æœ€å¾Œã®Run Stepã‹ã‚‰File Searchã®å®Ÿè¡Œçµæžœã‚’å«ã‚ã¦å–å¾—ã™ã‚‹
    run_step = client.beta.threads.runs.steps.retrieve(
        thread_id=thread_id,
        run_id=run_id,
        step_id=run_steps.data[-1].id,
        include=["step_details.tool_calls[*].file_search.results[*].content"]
    )
    if hasattr(run_step.step_details, "tool_calls"):
        file_search_tcs = [tc for tc in run_step.step_details.tool_calls if hasattr(tc, "file_search")]
        if file_search_tcs:
            return file_search_tcs[0].file_search.results
    print(run_step)
    return []
#    for result in run_step.step_details.tool_calls[0].file_search.results:
#        print(f""">>>>>>>>>>>>>>>>>>>>
#score: {result.score}
#fileId: {result.file_id}
#fileName: {result.file_name}
#content: {result.content[0].text}
#<<<<<<<<<<<<<<<<<<<<
#""")

def get_token_summery(response, model):
    if isinstance(response, ChatCompletion) or isinstance(response, Run):
        response = response.model_dump()
    token_summery = ""
    if "usage" in response and reduce(
        lambda a, c:c in response["usage"] and a,
        ["completion_tokens", "prompt_tokens", "total_tokens"], True):
        usage = response["usage"]
        token_summery = f"tokens in:{usage["prompt_tokens"]} out:{usage["completion_tokens"]} total:{usage["total_tokens"]}"
        if "pricing" in model:
            token_summery += f" cost: US${(usage["prompt_tokens"] * model["pricing"]["in"] + usage["completion_tokens"] * model["pricing"]["out"]) / 1000000}"
        token_summery = f"\n:violet-background[{token_summery}]"

    return token_summery 

class completion_streaming_digester:
    def __init__(self, stream):
        self.stream = stream
        self.response = {}

#    def handle_completion_streaming(response, responseHandler):
    def generator(self):
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŠã³tool_callã®æ–­ç‰‡ã‚’å—ã‘å–ã‚ŠãªãŒã‚‰UIã«åæ˜ ã—ã€æœ€çµ‚çš„ã«å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¾©å…ƒã™ã‚‹
#        res={}
        for chunk in self.stream:
#       for chunk in response:
            print(chunk)
            if isinstance(chunk, ChatCompletionChunk):
                # OpenAIã®Compltion APIã®å ´åˆ
                chunk_dict = chunk.model_dump()
            else:
                # deepseekãªã©
                chunk_dict = chunk
            for key, value in chunk_dict.items():
                if key == "choices":
                    if key not in self.response:
                        self.response[key] = {}
                    for choice in value:
                        for ckey, cvalue in choice.items():
                            ci = choice["index"]
                            if ci not in self.response[key]:
                                self.response[key][ci] = {"content": "", "tool_calls": {}}
                            if ckey == "delta":
                                for dkey, dvalue in cvalue.items():
                                    if dkey == "content" and dvalue:
                                        self.response[key][ci][dkey] += dvalue
                                        # UIã«å‡ºåŠ›
                                        yield dvalue
#                                       responseHandler.on_text_delta({"value": dvalue}, None)
                                    elif dkey == "tool_calls" and dvalue:
                                        for tool_call in dvalue:
                                            for tkey, tvalue in tool_call.items():
                                                ti = tool_call["index"]
                                                if ti not in self.response[key][ci][dkey]:
                                                    self.response[key][ci][dkey][ti] = {"function": {"arguments":""}}
                                                if tkey == "function" and tvalue:
                                                    if "name" in tvalue and tvalue["name"]:
                                                        self.response[key][ci][dkey][ti][tkey]["name"] = tvalue["name"]
                                                    if "arguments" in tvalue and tvalue["arguments"]:
                                                        self.response[key][ci][dkey][ti][tkey]["arguments"] += tvalue["arguments"]
                                                elif tvalue:
                                                    self.response[key][ci][dkey][ti][tkey] = tvalue
                                    elif dvalue:
                                        self.response[key][ci][dkey] = dvalue
                            elif cvalue:
                                self.response[key][ci][ckey] = cvalue
                elif value:
                    self.response[key] = value
#         if chunk.choices and chunk.choices[0].delta.content:
#             responseHandler.on_text_delta({"value": chunk.choices[0].delta.content}, None)
#             content += chunk.choices[0].delta.content
#         if chunk.choices and chunk.choices[0].delta.tool_calls:
#             tools += chunk.choices[0].delta.tool_calls
#         if chunk.choices and chunk.choices[0].finish_reason:
#             finish_reason = chunk.choices[0].finish_reason
    # ä¸Šè¨˜ãƒžãƒ¼ã‚¸ä½œæ¥­ã®éƒ½åˆä¸Šdictã§è¡¨ç¾ã•ã‚ŒãŸé…åˆ—ã‚’listã«å¤‰æ›ã™ã‚‹
        choices = []
        print(self.response)
        for ci, cvalue in sorted(self.response["choices"].items(), key=lambda x:x[0]):
            tool_calls = []
            for ti, tvalue in sorted(cvalue["tool_calls"].items(), key=lambda x:x[0]):
                tool_calls.append(tvalue)
            cvalue["tool_calls"] = tool_calls
            choices.append(cvalue)
        self.response["choices"] = choices

#         return res

#def tool_list_to_tool_obj(tools):
#    # Initialize a dictionary with default values
#    tool_calls_dict = defaultdict(lambda: {"id": None, "function": {"arguments": "", "name": None}, "type": None})
#
#    # Iterate over the tool calls
#    for tool_call in tools:
#        # If the id is not None, set it
#        if tool_call.id is not None:
#            tool_calls_dict[tool_call.index]["id"] = tool_call.id
#
#        # If the function name is not None, set it
#        if tool_call.function.name is not None:
#            tool_calls_dict[tool_call.index]["function"]["name"] = tool_call.function.name
#
#        # Append the arguments
#        tool_calls_dict[tool_call.index]["function"]["arguments"] += tool_call.function.arguments
#
#        # If the type is not None, set it
#        if tool_call.type is not None:
#            tool_calls_dict[tool_call.index]["type"] = tool_call.type
#
#    # Convert the dictionary to a list
#    tool_calls_list = list(tool_calls_dict.values())
#
#    # Return the result
#    return ChatCompletionMessagea(**({"role": "assistant", "content": None, "tool_calls": tool_calls_list}))

def get_assistant(client, mode):
    # IF: https://platform.openai.com/docs/assistants/how-it-works/creating-assistants
    current_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S+00:00")

    if mode == "development":
        instructions=f"ã‚ãªãŸã¯æ±Žç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›žç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html]
    else:
        instructions=f"ã‚ãªãŸã¯æ±Žç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›žç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, internetAccess.html]

    name=f"æ±Žç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ({mode})"
    assistant = None
    assistants = client.beta.assistants.list(order='desc', limit="100")
    print(assistants)
    for i in assistants.data:
        if i.created_at < time.time() - 86400:
            client.beta.assistants.delete(assistant_id=i.id)
            time.sleep(.2)
        elif i.name == name:
            assistant = i

    if not assistant:
        assistant = client.beta.assistants.create(
            name=name,
            model="gpt-4o"
        )
        print(f"..{assistant}==")

    if not assistant:
        print(f"=={assistant}==")
    else:
        print(f"--{assistant}==")

    client.beta.assistants.update(
        assistant_id=assistant.id,
        instructions=instructions,
        tools=tools
    )

    return assistant.id

# åˆæœŸåŒ–
if "clients" not in st.session_state:
    st.session_state.clients = {
        "openai": AzureOpenAI(
            azure_endpoint = os.getenv("ENDPOINT_URL"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2024-12-01-preview"
        ),
        "deepseek": ChatCompletionsClient(
            endpoint=os.getenv("DEEPSEEK_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("DEEPSEEK_AZURE_INFERENCE_CREDENTIAL"))
        ),
        "phi4": ChatCompletionsClient(
            endpoint=os.getenv("PHI_4_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("PHI_4_AZURE_INFERENCE_CREDENTIAL"))
        )
    }
if "assistants" not in st.session_state:
    st.session_state.assistants = {
        "gpt-4o": get_assistant(st.session_state.clients["openai"], os.getenv("IASA_DEPLOYMENT_MODE", "development"))
    }
if "conversation" not in st.session_state:
    st.session_state.conversation = ConversationManager(st.session_state.clients, st.session_state.assistants)
conversation = st.session_state.conversation 

models = {
  "GPT-4o": {
    "model": "gpt-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "assistant",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": False,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
  "GPT-4o-nostream": {
    "model": "gpt-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "assistant",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": False,
    "support_tools": True,
    "streaming": False,
    "pricing": {"in": 2.5, "out":10}
  },
  "GPT-4o-completion": {
    "model": "gpt-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
  "o3-mini": {
    "model": "o3-mini",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": False,
    "support_tools": True,
    "support_reasoning_effort": True,
    "streaming": True,
    "pricing": {"in": 1.1, "out":4.4}
  },
  "o1": {
    "model": "o1",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "streaming": False,
    "pricing": {"in": 15, "out":60}
  },
  "DeepSeek R1": {
    "model": "DeepSeek-R1",
    "client": st.session_state.clients["deepseek"],
    "api_mode": "inference",
    "streaming": True,
    "pricing": {"in": 0, "out":0}
  },
  "Microsoft Phi-4": {
    "model": "Phi-4",
    "client": st.session_state.clients["phi4"],
    "api_mode": "inference",
    "streaming": True,
    "pricing": {"in": 0, "out":0}
  }
}

#if "model" not in st.session_state:
#    st.session_state.model = models[list(models.keys())[0]]
if "fileCache" not in st.session_state:
    st.session_state.fileCache = {}
if "processing" not in st.session_state:
    st.session_state.processing = False
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = 0
if 'switches' not in st.session_state:
    st.session_state.switches = {
        "code_interpreter": True,
        "file_search": True,
        "get_google_results": True,
        "parse_html_content": True
    }

# ãƒ¡ã‚¤ãƒ³UI
st.title("Dual API Chat Interface")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    model = models[st.selectbox(
        "Model",
        models.keys()
    )]
    options = {}

#    st.session_state.model = model
    st.text("API Mode: " + model["api_mode"])
    st.text("Streaming: " + ("True" if model.get("streaming", False) else "False"))
    st.text("Support vision: " + ("True" if model.get("support_vision", False) else "False"))

    if model.get("support_reasoning_effort", False):
        options["reasoning_effort"] = st.selectbox(
            "reasoning_effort",
            ["low", "medium", "high"],
            index = 2
        )

    uploaded_files = None
    image_files = None
    if model["api_mode"] == "assistant":
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            accept_multiple_files=True,
            key = f"file_uploader_{st.session_state.uploader_key}"
        )
        tool_for_files = st.selectbox(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”¨é€”",
            ["file_search", "code_interpreter"]
        )
        st.session_state.switches[tool_for_files] = True

    if model.get("support_vision", False):
        image_files = st.file_uploader(
            "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type = ["png", "jpeg", "jpg", "webp", "gif"],
            key = f"image_uploader_{st.session_state.uploader_key}"
        )
        detail_level = st.selectbox(
            "ç”»åƒã®è©³ç´°ãƒ¬ãƒ™ãƒ«",
            ["auto", "high", "low"]
        )

    # "code_interpreter", "file_search"ã‚’ä½¿ãˆã‚‹ã®ã¯assistantã®æ™‚ã ã‘
    if model["api_mode"] != "assistant":
        tools = [tool for tool in tools if tool.get("type", None) == "function"]

    # è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
    tool_names = [
        t.get("type") if t.get("type") != "function"
        else t["function"]["name"]
        for t in tools
    ]

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒˆã‚°ãƒ«ã‚¹ã‚¤ãƒƒãƒã‚’è¡¨ç¤º
    if model.get("support_tools", False):
        st.header("ãƒ„ãƒ¼ãƒ«é¸æŠž")
        switches = {
            name: st.toggle(
                name,
                value=st.session_state.switches[name] if name in st.session_state.switches else False,
                key=f"tool_switch_{name}"
            )
            for name in tool_names
        }
        st.session_state.switches = st.session_state.switches | switches

        # é¸æŠžã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        selected_tools = [
            tool
            for i, tool in enumerate(tools)
            if st.session_state.switches[tool_names[i]]
        ]
    else:
        selected_tools = []
    print(selected_tools)
#    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒˆã‚°ãƒ«ã‚¹ã‚¤ãƒƒãƒã‚’è¡¨ç¤º
#    if model.get("support_tools", False):
#        st.header("ãƒ„ãƒ¼ãƒ«é¸æŠž")
#        switches = [
#            st.toggle(
#                tool_names[i],
#                value=False,
#                key=f"tool_switch_{i}"
#            )
#            for i in range(len(tool_names))
#        ]
#
#        # é¸æŠžã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
#        selected_tools = [
#            tool
#            for i, tool in enumerate(tools)
#            if switches[i]
#        ]
#    else:
#        selected_tools = []

# if not st.session_state.get("processing"):
#     # å‡¦ç†ã™ã¹ããƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç„¡ã„: æ¬¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ±‚ã‚ã‚‹
if content := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›"):
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        attachments = []
        if uploaded_files:
            st.toast("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            attachments = conversation.create_attachments_for_assistant(uploaded_files, tool_for_files)
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠžã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢

        if image_files:
            st.toast("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]
            for file in image_files:
#               # Completion APIã§ã¯ImageURLãŒã€Assistant APIã§ã¯ImageFileãŒç”¨ã„ã‚‰ã‚Œã‚‹
# ã“ã‚Œã¯ãŸã¶ã‚“é–“é•ã„ã§ã€Assistant APIã§ã¯ä¸¡æ–¹ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãš
#               # ä¸¡æ–¹ä½œæˆã—ã¦ãŠãã€APIå®Ÿè¡Œæ™‚ã«ä¸è¦ãªæ–¹ã¯æ¨ã¦ã‚‹
#               # ãŸã ã—ã€Threadæœªä½œæˆæ™‚ã«ã¯ImageURLã®ã¿ä½œæˆã¨ã—ã€Threadä½œæˆæ™‚ã«ImageURLã‹ã‚‰å¤‰æ›ç”Ÿæˆã™ã‚‹
# ã©ã†ã‚„ã‚‰gpt-4oã¯Assistant APIæ™‚ã«ã¯ç”»åƒã‚’èªè­˜ã§ããªã„æ¨¡æ§˜ï¼ˆæœ¬äººè«‡ï¼‰
# ç¾çŠ¶ã€Assistant API, Visionä¸¡å¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ãŒä»–ã«ãªã„ã®ã§ã€ä¸€æ—¦ã‚ãã‚‰ã‚ã‚‹ã€‚
                content.append(conversation.create_ImageURLContentBlock(file,detail_level))
#               if conversation.thread.thread_id or model["api_mode"] == "assistant":
#                   content.append(conversation.create_ImageFileContentBlock(file,detail_level))
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠžã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
    
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        conversation.add_message(model, "user", content, attachments)
        # å‡¦ç†ä¸­ã¸ç§»è¡Œ
        st.session_state.processing = True
        st.rerun()  # ã“ã“ã§ä¸€æ—¦å†æç”»(ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªã‚¢ãªã©ã«å¿…è¦)
    
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
pretty_print(conversation.thread.messages)

if st.session_state.get("processing"):
    # å‡¦ç†ã™ã¹ããƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹

    # APIå®Ÿè¡Œ
    try:
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        with st.chat_message("assistant"):
#            response_placeholder = st.container()

            content, metadata = execute_api(
                model,
                selected_tools,
                conversation,
                options
#                response_placeholder
                )
        
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å±¥æ­´ã«è¿½åŠ 
            conversation.add_message(model, "assistant", content, None, metadata)
            st.session_state.processing = False

            # æœ€çµ‚çš„ãªå†…å®¹ã§æç”»ã—ãªãŠã™
            # response_placeholdeãŒst.empty()ãªã‚‰ã€withå¥ã®container()ã«ã‚ˆã‚Šæ–°ãŸãªç©ºã®ã‚³ãƒ³ãƒ†ãƒŠãŒä½œæˆã•ã‚Œã‚‹ã¯ãšãªã®ã ãŒã€
            # ãƒ–ãƒ©ã‚¦ã‚¶å´ã¨ã®åŒæœŸã«å¤±æ•—ã—ã€å‰ã®ã‚³ãƒ³ãƒ†ãƒŠã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ®‹ã£ã¦ã—ã¾ã†ã€‚ã“ã‚Œã¯ã†ã¾ãã„ã‹ãªã„ã€‚
            # streamlitã§ã¯æ—¢ã«æç”»ã—ãŸã‚‚ã®ã‚’å¤‰æ›´ã™ã‚‹ã®ã¯ã‚„ã‚ãŸæ–¹ãŒã„ã„ã‹ã‚‚ã€‚ç´ ç›´ã«rerun()ã—ãŸæ–¹ãŒã„ã„ã€‚
#            with response_placeholder.container(key=f"last_msg{time.time()}"):
#                pretty_print_message("last_msg", conversation.thread.get_last_message(), with_token_summery=True)
#        st.rerun()
    
    except Exception as e:
        st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
#        st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n{traceback.format_exc()}")
        st.exception(e)
        st.button("ãƒªãƒˆãƒ©ã‚¤")
